{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This file contains the ANN method with $l_q=14$.\n",
    "\n",
    "1. The hyperparameter searching and training process is commented out for reproduction. Currently, the best hyperparameters (***for Supplementary Table 1***) and the trained models are stored in `/Reproduction/best_hps.csv` and `/Reproduction/TrainedModels/`, respectively. Please have a look at Section 3.5 and Section 4 of README for more information. If the hyperparameter searching and training process is restored, the best hyperparameters will be stored in `/Reproduction/Results/ANN/best_hps.csv` and the trained models will be stored in `/Reproduction/Results/ANN/TrainedModels/`.\n",
    "\n",
    "2. The results will be stored in `/Reproduction/Results/ANN/`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import keras_tuner as kt\n",
    "import tensorflow as tf\n",
    "import datetime\n",
    "import csv\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "pd.options.mode.chained_assignment = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# This file requires TF = 2.9.0\n",
    "print(tf.config.list_physical_devices('GPU'), tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the current directory.\n",
    "current_dir = os.getcwd()\n",
    "\n",
    "# Set up data source and hyperparameters.\n",
    "path = current_dir+'/Data14/'\n",
    "template_length = 14\n",
    "\n",
    "# Basic fuctions for generating shuffled training and validation data.\n",
    "def split_sequence(sequence, n_steps_infunction):\n",
    "    X, y = list(), list()\n",
    "    for i in range(len(sequence)):\n",
    "        end_ix = i + n_steps_infunction\n",
    "        if end_ix > len(sequence)-1:\n",
    "            break\n",
    "        seq_x, seq_y = sequence[i:end_ix], sequence[end_ix]\n",
    "        X.append(seq_x)\n",
    "        y.append(seq_y)\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "def shuffle_xy(x1, y1):\n",
    "    np.random.seed(0)\n",
    "    assert len(x1) == len(y1)\n",
    "    p = np.random.permutation(len(x1))\n",
    "    return x1[p], y1[p]\n",
    "\n",
    "# Set up random seeds for data splitting.\n",
    "split_rs = [290, 150, 266, 78, 148, 133, 155, 135, 178, 241]\n",
    "\n",
    "# Set up directories to store result data.\n",
    "for rs in split_rs:\n",
    "    os.makedirs(current_dir+'/Results/ANN/'+str(rs)+'-ResultData/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up data source and hyperparameters.\n",
    "path = current_dir+'/Data14/'\n",
    "template_length = 14\n",
    "this_m = 200\n",
    "this_w3 = 1.1\n",
    "\n",
    "for rs in split_rs:\n",
    "    ## Splitting\n",
    "    # Full set --> full training set and test set\n",
    "    print('Calculating seed', rs, 'at', datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S'))\n",
    "        \n",
    "    files = sorted(os.listdir(path))\n",
    "    pads = [elt[:-7] for elt in files]\n",
    "    files_df = pd.DataFrame({'filename':files, 'pad':pads})\n",
    "\n",
    "    num_wells_in_pad_df = pd.DataFrame(files_df['pad'].value_counts()).reset_index()\n",
    "    num_wells_in_pad_df.columns = ['pad', 'count']\n",
    "    unique_pads = np.unique(pads)\n",
    "    unique_pads_df = pd.DataFrame({'pad':unique_pads})\n",
    "    unique_pads_df = pd.merge(unique_pads_df, num_wells_in_pad_df, on='pad')\n",
    "\n",
    "    np.random.seed(rs)\n",
    "    unique_pads_df_shuffled = unique_pads_df.sample(frac=1).reset_index(drop=True)\n",
    "    counter = 0\n",
    "    for idx in range(len(unique_pads_df_shuffled)):\n",
    "        counter += unique_pads_df_shuffled['count'][idx]\n",
    "        if counter >= 300:\n",
    "            break\n",
    "        else:\n",
    "            continue\n",
    "    end_of_training = idx\n",
    "\n",
    "    train_files_shuffled = []\n",
    "    for idx in range(end_of_training+1):\n",
    "        pad_name = unique_pads_df_shuffled['pad'][idx]\n",
    "        for file in files:\n",
    "            if file[:-7] == pad_name:\n",
    "                train_files_shuffled.append(file)\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "    test_files_shuffled = []\n",
    "    for idx in range(end_of_training+1,len(unique_pads_df_shuffled)):\n",
    "        pad_name = unique_pads_df_shuffled['pad'][idx]\n",
    "        for file in files:\n",
    "            if file[:-7] == pad_name:\n",
    "                test_files_shuffled.append(file)\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "    # Full training set --> training set and validation set\n",
    "    train_files_shuffled_2, val_files_shuffled = [], []\n",
    "    for idx in range(1, len(train_files_shuffled)-1):\n",
    "        if idx >= 239:\n",
    "            if (train_files_shuffled[idx][:-7] == train_files_shuffled[idx-1][:-7]) and (train_files_shuffled[idx][:-7] != train_files_shuffled[idx+1][:-7]):\n",
    "                end_of_validation = idx\n",
    "                break\n",
    "    \n",
    "    for idx in range(len(train_files_shuffled)):\n",
    "        if idx <= end_of_validation:\n",
    "            train_files_shuffled_2.append(train_files_shuffled[idx])\n",
    "        else:\n",
    "            val_files_shuffled.append(train_files_shuffled[idx])\n",
    "    train_files_shuffled = train_files_shuffled_2\n",
    "\n",
    "    print('len(train_files_shuffled):', len(train_files_shuffled),'          len(val_files_shuffled):', len(val_files_shuffled), '          len(test_files_shuffled):', len(test_files_shuffled))\n",
    "    print(test_files_shuffled)\n",
    "\n",
    "    ## Calculate training data.\n",
    "    print('Calculating training matrix', 'at', datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S'))\n",
    "    x_train, y_train = np.empty([1, template_length]), np.empty([1,])\n",
    "\n",
    "    for m in range(len(train_files_shuffled)):\n",
    "            \n",
    "        df = pd.read_excel(path+train_files_shuffled[m], header = 0, sheet_name = 0)\n",
    "        reopenings = list(df[df['Mark'] == 'reopening'].index)\n",
    "        reopenings = np.insert(reopenings, len(reopenings), len(df))\n",
    "        df = df['Q']/df['t']\n",
    "\n",
    "        for l in range(len(reopenings)-1):\n",
    "            sub_df = df.iloc[reopenings[l]:reopenings[l+1]]\n",
    "            x_train_new, y_train_new = split_sequence(sub_df.values, template_length)\n",
    "            x_train = np.concatenate((x_train, x_train_new), axis=0)\n",
    "            y_train = np.concatenate((y_train, y_train_new), axis=0)\n",
    "\n",
    "    x_train, y_train = x_train[1:], y_train[1:]\n",
    "    x_train, y_train = shuffle_xy(x_train, y_train)\n",
    "\n",
    "    ## Calculate validation data.\n",
    "    print('Calculating validation matrix', 'at', datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S'))\n",
    "    x_val, y_val = np.empty([1, template_length]), np.empty([1,])\n",
    "\n",
    "    for m in range(len(val_files_shuffled)):\n",
    "            \n",
    "        df = pd.read_excel(path+val_files_shuffled[m], header = 0, sheet_name = 0)\n",
    "        reopenings = list(df[df['Mark'] == 'reopening'].index)\n",
    "        reopenings = np.insert(reopenings, len(reopenings), len(df))\n",
    "        df = df['Q']/df['t']\n",
    "\n",
    "        for l in range(len(reopenings)-1):\n",
    "            sub_df = df.iloc[reopenings[l]:reopenings[l+1]]\n",
    "            x_val_new, y_val_new = split_sequence(sub_df.values, template_length)\n",
    "            x_val = np.concatenate((x_val, x_val_new), axis=0)\n",
    "            y_val = np.concatenate((y_val, y_val_new), axis=0)\n",
    "                    \n",
    "    x_val, y_val = x_val[1:], y_val[1:]\n",
    "    x_val, y_val = shuffle_xy(x_val, y_val)\n",
    "\n",
    "    print('len(x_train):', x_train.shape, '          len(y_train):', y_train.shape, '          len(x_val):', x_val.shape, '          len(y_val):', y_val.shape)\n",
    "    S = StandardScaler().fit(x_train)\n",
    "    x_train = S.transform(x_train)\n",
    "    x_val = S.transform(x_val)\n",
    "\n",
    "    ## Model training.\n",
    "    tf.keras.utils.set_random_seed(317)\n",
    "    tf.config.experimental.enable_op_determinism()\n",
    "\n",
    "    # The hyperparameter searching and training process is commented out for reproduction.\n",
    "    '''def buildModel(hp):\n",
    "        model_input = layers.Input(shape=(template_length, 1))\n",
    "        lstm = layers.LSTM(hp.Choice('lstm_units', [75, 100, 125, 150, 175, 200]), input_shape=(template_length, 1))(model_input)\n",
    "        dense = layers.Dense(hp.Choice('dense_units', [10, 20, 30, 40, 50]), activation='relu')(lstm)\n",
    "        model_output = layers.Dense(1, activation='relu')(dense)\n",
    "\n",
    "        model = tf.keras.models.Model(model_input, model_output)\n",
    "        opt = tf.keras.optimizers.Adam(hp.Choice('lr', [0.01, 0.001, 0.0001]))\n",
    "        model.compile(optimizer=opt, loss=tf.keras.losses.MeanSquaredError())\n",
    "            \n",
    "        return model\n",
    "    \n",
    "    tuner = kt.Hyperband(\n",
    "        buildModel,\n",
    "        objective='val_loss',\n",
    "        max_epochs=1000,\n",
    "        directory=current_dir+'/Results/ANN/tuner-'+str(rs)\n",
    "        )\n",
    "    \n",
    "    tuner_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=20)\n",
    "    print('Searching best hyperparameters', 'at', datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S'))\n",
    "    tuner.search(x_train, y_train, epochs=1000, batch_size=256, validation_data=(x_val, y_val), callbacks=[tuner_stopping], verbose=0)\n",
    "    best_hps = tuner.get_best_hyperparameters()[0]\n",
    "    print(best_hps.values)\n",
    "\n",
    "    with open(current_dir+'/Results/ANN/best_hps.csv','a') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow([rs, best_hps.values['lstm_units'], best_hps.values['dense_units'], best_hps.values['lr']])\n",
    "        f.close()\n",
    "\n",
    "    print('Training', 'at', datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S'))\n",
    "    stopping = tf.keras.callbacks.EarlyStopping(\n",
    "        monitor = 'val_loss',\n",
    "        patience = 20\n",
    "    )\n",
    "\n",
    "    checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
    "        filepath = current_dir+'/Results/ANN/TrainedModels/ANN_model_' + str(rs) +'.h5',\n",
    "        monitor = 'val_loss',\n",
    "        save_best_only = True\n",
    "    )\n",
    "    \n",
    "    lstm_model = tuner.hypermodel.build(best_hps)\n",
    "    history = lstm_model.fit(x_train, y_train, epochs = 1000, batch_size = 256, validation_data = (x_val, y_val), callbacks = [checkpoint, stopping], verbose=0)\n",
    "    \n",
    "    trained_lstm_model = tf.keras.models.load_model(current_dir+'/Results/ANN/TrainedModels/ANN_model_' + str(rs) +'.h5')'''\n",
    "\n",
    "    # If the previous hyperparameter searching and training process is restored, please comment out the following line:\n",
    "    trained_lstm_model = tf.keras.models.load_model(current_dir+'/TrainedModels/ANN_model_' + str(rs) +'.h5')\n",
    "\n",
    "    ## Forecasting\n",
    "    print('Forecasting', 'at', datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S'))\n",
    "    for m in range(len(test_files_shuffled)):\n",
    "        \n",
    "        df = pd.read_excel(path+test_files_shuffled[m], header = 0, sheet_name = 0)\n",
    "        df['q'] = df['Q']/df['t']\n",
    "        \n",
    "        print('=====Calculating well=====', m, test_files_shuffled[m], 'at', datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S'))\n",
    "        reopenings = list(df[df['Mark'] == 'reopening'].index)\n",
    "        reopenings = np.insert(reopenings, len(reopenings), len(df))\n",
    "\n",
    "        forecasts_multisteps_this_well = []\n",
    "        y_true_all_this_well = []\n",
    "        prod_times_this_well = []\n",
    "        markers_this_well = []\n",
    "\n",
    "        for l in range(len(reopenings)-1):\n",
    "            sub_df = df.iloc[reopenings[l]:reopenings[l+1]] \n",
    "\n",
    "            y_true_all = sub_df['q'].values\n",
    "            forecasts_multisteps = list(sub_df['q'][:template_length].values)\n",
    "            prod_times = sub_df['t'].values\n",
    "            markers = ['initial'] * template_length\n",
    "            \n",
    "            history = sub_df['q'][:template_length].values\n",
    "            for t in range(len(sub_df)-template_length):\n",
    "                markers.append('forecast')\n",
    "                history_data = history.reshape(1, template_length)\n",
    "                forecast = trained_lstm_model.predict(S.transform(history_data), verbose=0)[0][0]\n",
    "                forecasts_multisteps.append(forecast)\n",
    "                history = np.append(history, forecast)\n",
    "                history = np.delete(history, 0)\n",
    "\n",
    "            for t in range(len(markers)):\n",
    "                y_true_all_this_well.append(y_true_all[t])\n",
    "                forecasts_multisteps_this_well.append(forecasts_multisteps[t])\n",
    "                prod_times_this_well.append(prod_times[t])\n",
    "                markers_this_well.append(markers[t])\n",
    "\n",
    "        # Results\n",
    "        multi_step_result_df = pd.DataFrame()\n",
    "        multi_step_result_df['True'] = y_true_all_this_well\n",
    "        multi_step_result_df['Pred'] = forecasts_multisteps_this_well\n",
    "        multi_step_result_df['t'] = prod_times_this_well\n",
    "        multi_step_result_df['Mark'] = markers_this_well\n",
    "        multi_step_result_df['TrueCumu'] = (multi_step_result_df['True']*multi_step_result_df['t']).cumsum()\n",
    "        multi_step_result_df['PredCumu'] = (multi_step_result_df['Pred']*multi_step_result_df['t']).cumsum()\n",
    "\n",
    "        writer = pd.ExcelWriter(current_dir+'/Results/ANN/'+str(rs)+'-ResultData/ResultData-'+str(m)+'-'+test_files_shuffled[m])\n",
    "        multi_step_result_df.to_excel(writer, float_format='%.5f', header=True, index=False)\n",
    "        writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dtw1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
