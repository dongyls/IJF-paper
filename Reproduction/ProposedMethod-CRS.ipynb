{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This file contains the default proposed method with 9 new clustering seeds.\n",
    "\n",
    "1. The best number of clusters will be printed out during execution.\n",
    "\n",
    "2. The results will be stored in `/Reproduction/Results/CRS/`. Inside this folder, the second-layer folders will be named by the random seeds for data splitting. Inside each of these folders, the third-layer folders contains the raw results of the proposed method with 9 new clustering seeds. For example, `/Reproduction/Results/CRS/0/0-1-ResultData/` is the folder for the results of the proposed method with splitting seed 0 and clustering seed 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sklearn\n",
    "import sklearn.cluster\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "from kneed import KneeLocator\n",
    "\n",
    "pd.options.mode.chained_assignment = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the current directory.\n",
    "current_dir = os.getcwd()\n",
    "\n",
    "# Basic fuction for generating query and template series.\n",
    "def split_sequence_with_normalisation(sequence, n_steps_infunction):\n",
    "    X, y, sequence_mean, sequence_std = list(), list(), list(), list()\n",
    "    for i in range(len(sequence)):\n",
    "        end_ix = i + n_steps_infunction\n",
    "        if end_ix > len(sequence)-1:\n",
    "            break\n",
    "        seq_x_original, seq_y = sequence[i:end_ix], sequence[end_ix]\n",
    "        seq_x_mean, seq_x_std = np.mean(seq_x_original), np.std(seq_x_original)\n",
    "        seq_x = (seq_x_original - seq_x_mean) / seq_x_std\n",
    "        X.append(seq_x)\n",
    "        y.append(seq_y)\n",
    "        sequence_mean.append(seq_x_mean)\n",
    "        sequence_std.append(seq_x_std)\n",
    "    return np.array(X), np.array(y), np.array(sequence_mean), np.array(sequence_std)\n",
    "\n",
    "# Set up random seeds for data splitting and clustering.\n",
    "split_rs = [290, 150, 266, 78, 148, 133, 155, 135, 178, 241]\n",
    "new_cluster_rs = [1, 186, 333, 70, 63, 298, 159, 3, 62]\n",
    "\n",
    "# Set up directories to store result data.\n",
    "for rs in split_rs:\n",
    "    for this_crs in new_cluster_rs:    \n",
    "        os.makedirs(current_dir+'/Results/CRS/'+str(rs)+'/'+str(rs)+'-'+str(this_crs)+'-ResultData/')\n",
    "\n",
    "# Set up data source and hyperparameters.\n",
    "path = current_dir+'/Data14/'\n",
    "template_length = 14\n",
    "query_length = 14\n",
    "this_m = 200\n",
    "this_w3 = 1.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for rs in split_rs:\n",
    "    print('Calculating seed', rs, 'at', datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S'))\n",
    "    for this_crs in new_cluster_rs:\n",
    "        ## Splitting data\n",
    "        print('Calculating clustering seed', this_crs, 'at', datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S'))\n",
    "            \n",
    "        files = sorted(os.listdir(path))\n",
    "        pads = [elt[:-7] for elt in files]\n",
    "        files_df = pd.DataFrame({'filename':files, 'pad':pads})\n",
    "\n",
    "        num_wells_in_pad_df = pd.DataFrame(files_df['pad'].value_counts()).reset_index()\n",
    "        num_wells_in_pad_df.columns = ['pad', 'count']\n",
    "        unique_pads = np.unique(pads)\n",
    "        unique_pads_df = pd.DataFrame({'pad':unique_pads})\n",
    "        unique_pads_df = pd.merge(unique_pads_df, num_wells_in_pad_df, on='pad')\n",
    "\n",
    "        np.random.seed(rs)\n",
    "        unique_pads_df_shuffled = unique_pads_df.sample(frac=1).reset_index(drop=True)\n",
    "        counter = 0\n",
    "        for idx in range(len(unique_pads_df_shuffled)):\n",
    "            counter += unique_pads_df_shuffled['count'][idx]\n",
    "            if counter >= 300:\n",
    "                break\n",
    "            else:\n",
    "                continue\n",
    "        end_of_training = idx\n",
    "\n",
    "        train_files_shuffled = []\n",
    "        for idx in range(end_of_training+1):\n",
    "            pad_name = unique_pads_df_shuffled['pad'][idx]\n",
    "            for file in files:\n",
    "                if file[:-7] == pad_name:\n",
    "                    train_files_shuffled.append(file)\n",
    "                else:\n",
    "                    continue\n",
    "\n",
    "        test_files_shuffled = []\n",
    "        for idx in range(end_of_training+1,len(unique_pads_df_shuffled)):\n",
    "            pad_name = unique_pads_df_shuffled['pad'][idx]\n",
    "            for file in files:\n",
    "                if file[:-7] == pad_name:\n",
    "                    test_files_shuffled.append(file)\n",
    "                else:\n",
    "                    continue\n",
    "\n",
    "        print('len(train_files_shuffled):', len(train_files_shuffled), '          len(test_files_shuffled):', len(test_files_shuffled))\n",
    "        print(test_files_shuffled)\n",
    "\n",
    "        ## Calculating template set\n",
    "        print('Calculating template_df', 'at', datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S'))\n",
    "        template, forecast, template_mean, template_std = np.empty([1, template_length]), np.empty([1,]), np.empty([1,]), np.empty([1,])\n",
    "\n",
    "        for i in range(len(train_files_shuffled)):\n",
    "            temp_df = pd.read_excel(path+train_files_shuffled[i], header = 0, sheet_name = 0)\n",
    "            reopenings = list(temp_df[temp_df['Mark'] == 'reopening'].index)\n",
    "            reopenings = np.insert(reopenings, len(reopenings), len(temp_df))\n",
    "            temp_df = temp_df['Q']/temp_df['t']\n",
    "            for j in range(len(reopenings)-1):\n",
    "                temp_sub_df = temp_df.iloc[reopenings[j]:reopenings[j+1]]\n",
    "                temp_template, temp_forecast, temp_template_mean, temp_template_std = split_sequence_with_normalisation(temp_sub_df.values, template_length)\n",
    "                template = np.concatenate((template, temp_template), axis=0)\n",
    "                forecast = np.concatenate((forecast, temp_forecast), axis=0)\n",
    "                template_mean = np.concatenate((template_mean, temp_template_mean), axis=0)\n",
    "                template_std = np.concatenate((template_std, temp_template_std), axis=0)\n",
    "\n",
    "        template, forecast, template_mean, template_std = template[1:], forecast[1:], template_mean[1:], template_std[1:]\n",
    "        template_df = pd.DataFrame(template)\n",
    "        forecast_df = pd.DataFrame(forecast)\n",
    "        template_mean_df = pd.DataFrame(template_mean)\n",
    "        template_std_df = pd.DataFrame(template_std)\n",
    "        print(template_df.shape, forecast_df.shape, template_mean_df.shape, template_std_df.shape)\n",
    "\n",
    "        ## Clustering\n",
    "        print('Searching n_clusters', 'at', datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S'))\n",
    "        scores = []\n",
    "        n_clusters_to_search = np.linspace(50, 600, 12, dtype=int)\n",
    "        for n_of_clusters in n_clusters_to_search:\n",
    "            print('n_clusters=', n_of_clusters, 'at', datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S'))\n",
    "            kmeans = sklearn.cluster.KMeans(n_clusters=n_of_clusters, max_iter=10000, tol=1e-6, random_state=this_crs).fit(template_df.values)\n",
    "            scores.append(kmeans.inertia_)\n",
    "        kn = KneeLocator(n_clusters_to_search, scores, curve='convex', direction='decreasing')\n",
    "        best_n_clusters = kn.knee\n",
    "        print('best_n_clusters:', best_n_clusters)\n",
    "\n",
    "        print('Clustering', 'at', datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S'))\n",
    "        kmeans = sklearn.cluster.KMeans(n_clusters=best_n_clusters, max_iter=10000, tol=1e-6, random_state=this_crs).fit(template_df.values)\n",
    "        labels = kmeans.labels_\n",
    "        clustering_result = pd.DataFrame()\n",
    "        clustering_result['Template'] = template_df.index\n",
    "        clustering_result['Cluster'] = labels\n",
    "        for i in range(len(clustering_result)):\n",
    "            clustering_result['Cluster'][i] = 'Cluster '+str(clustering_result['Cluster'][i]+1)\n",
    "\n",
    "        cluster_centers_df = pd.DataFrame(kmeans.cluster_centers_)\n",
    "\n",
    "        ## Forecasting\n",
    "        print('Forecasting', 'at', datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S'))\n",
    "        # Get a test well.\n",
    "        for m in range(len(test_files_shuffled)):\n",
    "        \n",
    "            df = pd.read_excel(path+test_files_shuffled[m], header = 0, sheet_name = 0)\n",
    "            df['q'] = df['Q']/df['t']\n",
    "\n",
    "            print('=====Calculating well=====', m, test_files_shuffled[m], 'at', datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S'))\n",
    "            reopenings = list(df[df['Mark'] == 'reopening'].index)\n",
    "            reopenings = np.insert(reopenings, len(reopenings), len(df))\n",
    "\n",
    "            forecasts_multisteps_this_well = []\n",
    "            y_true_all_this_well = []\n",
    "            prod_times_this_well = []\n",
    "            markers_this_well = []\n",
    "\n",
    "            # Get a production stage.\n",
    "            for l in range(len(reopenings)-1):\n",
    "                sub_df = df.iloc[reopenings[l]:reopenings[l+1]]\n",
    "                \n",
    "                forecasts_multisteps = list(sub_df['q'][:template_length].values)\n",
    "                prod_times = sub_df['t'].values\n",
    "                markers = ['initial'] * template_length\n",
    "\n",
    "                sub_df = sub_df['q']\n",
    "                global_upper_bound = sub_df.iloc[:template_length].max() * this_w3\n",
    "                query_all, y_true_all, query_mean_all, query_std_all = split_sequence_with_normalisation(sub_df.values, query_length)\n",
    "                y_true_all_expanded = list(sub_df[:template_length].values) + list(y_true_all)\n",
    "                \n",
    "                query = query_all[0]\n",
    "                query_mean = query_mean_all[0]\n",
    "                query_std = query_std_all[0]\n",
    "                for j in range(len(query_all)):\n",
    "                    markers.append('forecast')\n",
    "\n",
    "                    distances_to_cluster = []\n",
    "                    for i in range(len(cluster_centers_df)):\n",
    "                        template = cluster_centers_df.iloc[i,:].values\n",
    "                        distances_to_cluster.append(np.linalg.norm(query-template))\n",
    "                    neighbour = sorted(range(len(distances_to_cluster)), key=lambda idx: distances_to_cluster[idx])[0]\n",
    "                    cluster_name_of_neighbour = 'Cluster '+str(neighbour+1)\n",
    "                    idx_of_templates_to_search = clustering_result[clustering_result['Cluster'] == cluster_name_of_neighbour]['Template'].values\n",
    "\n",
    "                    templates_to_search = template_df.iloc[idx_of_templates_to_search].reset_index(drop=True)\n",
    "                    templates_to_search_mean = template_mean_df.iloc[idx_of_templates_to_search].reset_index(drop=True)\n",
    "                    templates_to_search_std = template_std_df.iloc[idx_of_templates_to_search].reset_index(drop=True)\n",
    "                    templates_to_search_forecast = forecast_df.iloc[idx_of_templates_to_search].reset_index(drop=True)\n",
    "\n",
    "                    distances = []\n",
    "                    for i in range(len(templates_to_search)):\n",
    "                        template = templates_to_search.iloc[i,:].values\n",
    "                        distances.append(np.linalg.norm(query-template))\n",
    "\n",
    "                    neighbours = sorted(range(len(distances)), key=lambda idx: distances[idx])[:this_m]\n",
    "                    variations = []\n",
    "                    for i in range(len(neighbours)):\n",
    "                        template_normalised = templates_to_search.iloc[neighbours[i],:]\n",
    "                        this_template_mean = templates_to_search_mean.iloc[neighbours[i]].values\n",
    "                        this_template_std = templates_to_search_std.iloc[neighbours[i]].values\n",
    "                        this_forecast = templates_to_search_forecast.iloc[neighbours[i]].values\n",
    "                        template_original = template_normalised*this_template_std + this_template_mean\n",
    "                        \n",
    "                        template_weights = []\n",
    "                        template_length_sum = sum(range(1, template_length+1))\n",
    "                        for k in range(1, len(template_original)+1):\n",
    "                            template_weights.append(k/template_length_sum)\n",
    "                        template_weighted_sum =  np.sum([a*b for a,b in zip(template_weights,template_original)])\n",
    "                            \n",
    "                        variation = (this_forecast-template_weighted_sum)/template_weighted_sum\n",
    "                        variations.append(variation[0])\n",
    "\n",
    "                    distance_weights = []\n",
    "                    neighbour_distances = [distances[idx] for idx in neighbours]\n",
    "                    neighbour_distances_sum = np.sum(neighbour_distances)\n",
    "                    actual_num_neighbours = len(neighbours)\n",
    "                    for i in range(len(neighbour_distances)):\n",
    "                        distance_weights.append(neighbour_distances[actual_num_neighbours-1-i]/neighbour_distances_sum)\n",
    "                    variation_forecast = np.sum([a*b for a,b in zip(distance_weights,variations)])\n",
    "                    \n",
    "                    query_original = query*query_std + query_mean\n",
    "                    query_weights = []\n",
    "                    query_length_sum = sum(range(1, query_length+1))\n",
    "                    for k in range(1, len(query_original)+1):\n",
    "                        query_weights.append(k/query_length_sum)\n",
    "                    query_weighted_sum =  np.sum([a*b for a,b in zip(query_weights,query_original)])\n",
    "                    forecast = query_weighted_sum*variation_forecast + query_weighted_sum\n",
    "                    if forecast > global_upper_bound:\n",
    "                        forecast = query_weighted_sum\n",
    "                    forecasts_multisteps.append(forecast)\n",
    "                    \n",
    "                    query_original_new = np.append(query_original, forecast)\n",
    "                    query_original_new = np.delete(query_original_new, 0)\n",
    "                    query_mean, query_std = np.mean(query_original_new), np.std(query_original_new), \n",
    "                    if query_std == 0:\n",
    "                        break\n",
    "                    else:\n",
    "                        query = (query_original_new - query_mean)/query_std\n",
    "                \n",
    "                for j2 in range(j+1, len(query_all)):\n",
    "                    markers.append('forecast')\n",
    "                    forecasts_multisteps.append(query_mean)\n",
    "\n",
    "                for j in range(len(markers)):\n",
    "                    y_true_all_this_well.append(y_true_all_expanded[j])\n",
    "                    forecasts_multisteps_this_well.append(forecasts_multisteps[j])\n",
    "                    prod_times_this_well.append(prod_times[j])\n",
    "                    markers_this_well.append(markers[j])\n",
    "\n",
    "            # Result\n",
    "            multi_step_result_df = pd.DataFrame()\n",
    "            multi_step_result_df['True'] = y_true_all_this_well\n",
    "            multi_step_result_df['Pred'] = forecasts_multisteps_this_well\n",
    "            multi_step_result_df['t'] = prod_times_this_well\n",
    "            multi_step_result_df['Mark'] = markers_this_well\n",
    "            multi_step_result_df['TrueCumu'] = (multi_step_result_df['True']*multi_step_result_df['t']).cumsum()\n",
    "            multi_step_result_df['PredCumu'] = (multi_step_result_df['Pred']*multi_step_result_df['t']).cumsum()\n",
    "\n",
    "            writer = pd.ExcelWriter(current_dir+'/Results/CRS/'+str(rs)+'/'+str(rs)+'-'+str(this_crs)+'-ResultData/ResultData-'+str(m)+'-'+str(test_files_shuffled[m]))\n",
    "            multi_step_result_df.to_excel(writer, float_format='%.5f', header=True, index=False)\n",
    "            writer.save()\n",
    "            writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dtw1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
