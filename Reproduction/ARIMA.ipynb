{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This file contains ARIMA-R and ARIMA-P with $l_q=14$.\n",
    "\n",
    "1. Please have a look at this issue if you are interested: Error: Input contains NaN, infinity or a value too large for dtype('float64'): pmdarima.predict() #404 [https://github.com/alkaline-ml/pmdarima/issues/404]. We have implemented a workaround of this issue provided by the discussions in the above GitHub page. This corresponds to the try-except lines in the forecasting steps.\n",
    "\n",
    "2. `pmdarima.auto_arima()` may produce warnings during the fitting process. Some of the warnings prop out randomly according to our tests. Moreover, we have not found any differences in the results brought by these warnings during the tests, so they can be ignored.\n",
    "\n",
    "3. The results will be stored in `/Reproduction/Results/ARIMAR/` and `/Reproduction/Results/ARIMAP/`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pmdarima as pm\n",
    "import datetime\n",
    "\n",
    "pd.options.mode.chained_assignment = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the current directory.\n",
    "current_dir = os.getcwd()\n",
    "\n",
    "# Set up random seeds for data splitting.\n",
    "split_rs = [290, 150, 266, 78, 148, 133, 155, 135, 178, 241]\n",
    "\n",
    "# Set up directories to store result data.\n",
    "for rs in split_rs:\n",
    "    os.makedirs(current_dir+'/Results/ARIMAR/'+str(rs)+'-ResultData/')\n",
    "    os.makedirs(current_dir+'/Results/ARIMAP/'+str(rs)+'-ResultData/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ARIMA-R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up data source and hyperparameters.\n",
    "path = current_dir+'/Data14/'\n",
    "template_length = 14\n",
    "\n",
    "for rs in split_rs:\n",
    "    ## Splitting data\n",
    "    print('Calculating seed', rs, 'at', datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S'))\n",
    "        \n",
    "    files = sorted(os.listdir(path))\n",
    "    pads = [elt[:-7] for elt in files]\n",
    "    files_df = pd.DataFrame({'filename':files, 'pad':pads})\n",
    "\n",
    "    num_wells_in_pad_df = pd.DataFrame(files_df['pad'].value_counts()).reset_index()\n",
    "    num_wells_in_pad_df.columns = ['pad', 'count']\n",
    "    unique_pads = np.unique(pads)\n",
    "    unique_pads_df = pd.DataFrame({'pad':unique_pads})\n",
    "    unique_pads_df = pd.merge(unique_pads_df, num_wells_in_pad_df, on='pad')\n",
    "\n",
    "    np.random.seed(rs)\n",
    "    unique_pads_df_shuffled = unique_pads_df.sample(frac=1).reset_index(drop=True)\n",
    "    counter = 0\n",
    "    for idx in range(len(unique_pads_df_shuffled)):\n",
    "        counter += unique_pads_df_shuffled['count'][idx]\n",
    "        if counter >= 300:\n",
    "            break\n",
    "        else:\n",
    "            continue\n",
    "    end_of_training = idx\n",
    "\n",
    "    train_files_shuffled = []\n",
    "    for idx in range(end_of_training+1):\n",
    "        pad_name = unique_pads_df_shuffled['pad'][idx]\n",
    "        for file in files:\n",
    "            if file[:-7] == pad_name:\n",
    "                train_files_shuffled.append(file)\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "    test_files_shuffled = []\n",
    "    for idx in range(end_of_training+1,len(unique_pads_df_shuffled)):\n",
    "        pad_name = unique_pads_df_shuffled['pad'][idx]\n",
    "        for file in files:\n",
    "            if file[:-7] == pad_name:\n",
    "                test_files_shuffled.append(file)\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "    print('len(train_files_shuffled):', len(train_files_shuffled), '          len(test_files_shuffled):', len(test_files_shuffled))\n",
    "    print(test_files_shuffled)\n",
    "\n",
    "    ## Forecasting\n",
    "    print('Forecasting', 'at', datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S'))\n",
    "    # Get a test well.\n",
    "    for m in range(len(test_files_shuffled)):\n",
    "        \n",
    "        df = pd.read_excel(path+test_files_shuffled[m], header = 0, sheet_name = 0)\n",
    "        df['q'] = df['Q']/df['t']\n",
    "\n",
    "        print('=====Calculating well=====', m, test_files_shuffled[m], 'at', datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S'))\n",
    "        reopenings = list(df[df['Mark'] == 'reopening'].index)\n",
    "        reopenings = np.insert(reopenings, len(reopenings), len(df))\n",
    "\n",
    "        forecasts_multisteps_this_well = []\n",
    "        y_true_all_this_well = []\n",
    "        prod_times_this_well = []\n",
    "        markers_this_well = []\n",
    "\n",
    "        for l in range(len(reopenings)-1):\n",
    "            sub_df = df.iloc[reopenings[l]:reopenings[l+1]]\n",
    "            \n",
    "            forecasts_multisteps = list(sub_df['q'][:template_length].values)\n",
    "            prod_times = sub_df['t'].values\n",
    "            markers = ['initial'] * template_length\n",
    "            \n",
    "            y_true_all = sub_df['q'].values\n",
    "            history = sub_df['q'][:template_length].values\n",
    "\n",
    "            for j in range(len(sub_df)-template_length):\n",
    "                markers.append('forecast')\n",
    "                \n",
    "                try:\n",
    "                    model_fit = pm.auto_arima(history, start_p=1, start_q=1, max_p=5, max_q=5, seasonal=False, stepwise=False, error_action='ignore', n_jobs=-1, maxiter=100000, random=False, random_state=0)\n",
    "                    forecast = model_fit.predict(1)[0]\n",
    "                except:\n",
    "                    model_fit = pm.auto_arima(history*0.1, start_p=1, start_q=1, max_p=5, max_q=5, seasonal=False, stepwise=False, error_action='ignore', n_jobs=-1, maxiter=100000, random=False, random_state=0)\n",
    "                    forecast = model_fit.predict(1)[0]*10\n",
    "                \n",
    "                if forecast < 0:\n",
    "                    forecast = np.mean(history)\n",
    "                forecasts_multisteps.append(forecast)\n",
    "                history = np.append(history, forecast)\n",
    "                history = np.delete(history, 0)\n",
    "\n",
    "            for j in range(len(markers)):\n",
    "                y_true_all_this_well.append(y_true_all[j])\n",
    "                forecasts_multisteps_this_well.append(forecasts_multisteps[j])\n",
    "                prod_times_this_well.append(prod_times[j])\n",
    "                markers_this_well.append(markers[j])\n",
    "                    \n",
    "        # Result\n",
    "        multi_step_result_df = pd.DataFrame()\n",
    "        multi_step_result_df['True'] = y_true_all_this_well\n",
    "        multi_step_result_df['Pred'] = forecasts_multisteps_this_well\n",
    "        multi_step_result_df['t'] = prod_times_this_well\n",
    "        multi_step_result_df['Mark'] = markers_this_well\n",
    "        multi_step_result_df['TrueCumu'] = (multi_step_result_df['True']*multi_step_result_df['t']).cumsum()\n",
    "        multi_step_result_df['PredCumu'] = (multi_step_result_df['Pred']*multi_step_result_df['t']).cumsum()\n",
    "\n",
    "        writer = pd.ExcelWriter(current_dir+'/Results/ARIMAR/'+str(rs)+'-ResultData/ResultData-'+str(m)+'-'+str(test_files_shuffled[m]))\n",
    "        multi_step_result_df.to_excel(writer, float_format='%.5f', header=True, index=False)\n",
    "        writer.save()\n",
    "        writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ARIMA-P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up data source and hyperparameters.\n",
    "path = current_dir+'/Data14/'\n",
    "template_length = 14\n",
    "\n",
    "for rs in split_rs:\n",
    "    ## Splitting data\n",
    "    print('Calculating seed', rs, 'at', datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S'))\n",
    "        \n",
    "    files = sorted(os.listdir(path))\n",
    "    pads = [elt[:-7] for elt in files]\n",
    "    files_df = pd.DataFrame({'filename':files, 'pad':pads})\n",
    "\n",
    "    num_wells_in_pad_df = pd.DataFrame(files_df['pad'].value_counts()).reset_index()\n",
    "    num_wells_in_pad_df.columns = ['pad', 'count']\n",
    "    unique_pads = np.unique(pads)\n",
    "    unique_pads_df = pd.DataFrame({'pad':unique_pads})\n",
    "    unique_pads_df = pd.merge(unique_pads_df, num_wells_in_pad_df, on='pad')\n",
    "\n",
    "    np.random.seed(rs)\n",
    "    unique_pads_df_shuffled = unique_pads_df.sample(frac=1).reset_index(drop=True)\n",
    "    counter = 0\n",
    "    for idx in range(len(unique_pads_df_shuffled)):\n",
    "        counter += unique_pads_df_shuffled['count'][idx]\n",
    "        if counter >= 300:\n",
    "            break\n",
    "        else:\n",
    "            continue\n",
    "    end_of_training = idx\n",
    "\n",
    "    train_files_shuffled = []\n",
    "    for idx in range(end_of_training+1):\n",
    "        pad_name = unique_pads_df_shuffled['pad'][idx]\n",
    "        for file in files:\n",
    "            if file[:-7] == pad_name:\n",
    "                train_files_shuffled.append(file)\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "    test_files_shuffled = []\n",
    "    for idx in range(end_of_training+1,len(unique_pads_df_shuffled)):\n",
    "        pad_name = unique_pads_df_shuffled['pad'][idx]\n",
    "        for file in files:\n",
    "            if file[:-7] == pad_name:\n",
    "                test_files_shuffled.append(file)\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "    print('len(train_files_shuffled):', len(train_files_shuffled), '          len(test_files_shuffled):', len(test_files_shuffled))\n",
    "    print(test_files_shuffled)\n",
    "\n",
    "    ## Forecasting\n",
    "    print('Forecasting', 'at', datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S'))\n",
    "    # Get a test well.\n",
    "    for m in range(len(test_files_shuffled)):\n",
    "        \n",
    "        df = pd.read_excel(path+test_files_shuffled[m], header = 0, sheet_name = 0)\n",
    "        df['q'] = df['Q']/df['t']\n",
    "\n",
    "        print('=====Calculating well=====', m, test_files_shuffled[m], 'at', datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S'))\n",
    "        reopenings = list(df[df['Mark'] == 'reopening'].index)\n",
    "        reopenings = np.insert(reopenings, len(reopenings), len(df))\n",
    "\n",
    "        forecasts_multisteps_this_well = []\n",
    "        y_true_all_this_well = []\n",
    "        prod_times_this_well = []\n",
    "        markers_this_well = []\n",
    "\n",
    "        for l in range(len(reopenings)-1):\n",
    "            sub_df = df.iloc[reopenings[l]:reopenings[l+1]]\n",
    "            \n",
    "            forecasts_multisteps = list(sub_df['q'][:template_length].values)\n",
    "            prod_times = sub_df['t'].values\n",
    "            markers = ['initial'] * template_length\n",
    "            \n",
    "            y_true_all = sub_df['q'].values\n",
    "            history = sub_df['q'][:template_length].values\n",
    "\n",
    "            try:\n",
    "                model_fit = pm.auto_arima(history, start_p=1, start_q=1, max_p=5, max_q=5, seasonal=False, stepwise=False, error_action='ignore', n_jobs=-1, maxiter=100000, random=False, random_state=0)\n",
    "                forecast = model_fit.predict(len(sub_df)-template_length)\n",
    "            except:\n",
    "                model_fit = pm.auto_arima(history*0.1, start_p=1, start_q=1, max_p=5, max_q=5, seasonal=False, stepwise=False, error_action='ignore', n_jobs=-1, maxiter=100000, random=False, random_state=0)\n",
    "                forecast = model_fit.predict(len(sub_df)-template_length)*10\n",
    "                \n",
    "            all_forecast = np.array(list(y_true_all[:template_length])+list(forecast))\n",
    "            for j in range(len(sub_df)-template_length):\n",
    "                markers.append('forecast')\n",
    "                if all_forecast[template_length+j] < 0:\n",
    "                    all_forecast[template_length+j] = np.mean(all_forecast[j:template_length+j])\n",
    "                forecasts_multisteps.append(all_forecast[template_length+j])\n",
    "\n",
    "            for j in range(len(markers)):\n",
    "                y_true_all_this_well.append(y_true_all[j])\n",
    "                forecasts_multisteps_this_well.append(forecasts_multisteps[j])\n",
    "                prod_times_this_well.append(prod_times[j])\n",
    "                markers_this_well.append(markers[j])\n",
    "                    \n",
    "        # Result\n",
    "        multi_step_result_df = pd.DataFrame()\n",
    "        multi_step_result_df['True'] = y_true_all_this_well\n",
    "        multi_step_result_df['Pred'] = forecasts_multisteps_this_well\n",
    "        multi_step_result_df['t'] = prod_times_this_well\n",
    "        multi_step_result_df['Mark'] = markers_this_well\n",
    "        multi_step_result_df['TrueCumu'] = (multi_step_result_df['True']*multi_step_result_df['t']).cumsum()\n",
    "        multi_step_result_df['PredCumu'] = (multi_step_result_df['Pred']*multi_step_result_df['t']).cumsum()\n",
    "\n",
    "        writer = pd.ExcelWriter(current_dir+'/Results/ARIMAP/'+str(rs)+'-ResultData/ResultData-'+str(m)+'-'+str(test_files_shuffled[m]))\n",
    "        multi_step_result_df.to_excel(writer, float_format='%.5f', header=True, index=False)\n",
    "        writer.save()\n",
    "        writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dtw1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
